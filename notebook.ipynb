{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## This notebook contains sample code for running AMP with PyTorch in Gradient. For a worked tutorial, please see `amp_recipe.ipynb`"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Sample Workflow for working with AMP\n",
    "\n",
    "```\n",
    "from apex import amp\n",
    "model, optimizer = amp.initialize(model, optimizer, opt_level=\"O1\")\n",
    "\n",
    "\n",
    "loss = criterion(…)\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "\n",
    "\n",
    "loss = criterion(…)\n",
    "with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
    "    scaled_loss.backward()\n",
    "optimizer.step()\n",
    "\n",
    "```"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Capturing function calls"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "orig_linear = torch.nn.functional.linear\n",
    "def wrapped_linear(*args):\n",
    " casted_args = []\n",
    "  for arg in args:\n",
    "    if torch.is_tensor(arg) and torch.is_floating_point(arg):\n",
    "      casted_args.append(torch.cast(arg, torch.float16))\n",
    "    else:\n",
    "      casted_args.append(arg)\n",
    "  return orig_linear(*casted_args)\n",
    "torch.nn.functional.linear = wrapped_linear"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Autocasting and Gradient Scaling Using PyTorch"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Creates model and optimizer in default precision\n",
    "model = Net().cuda()\n",
    "optimizer = optim.SGD(model.parameters(), ...)\n",
    "\n",
    "# Creates a GradScaler once at the beginning of training.\n",
    "scaler = GradScaler()\n",
    "\n",
    "for epoch in epochs:\n",
    "    for input, target in data:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Runs the forward pass with autocasting.\n",
    "        with autocast(device_type='cuda', dtype=torch.float16):\n",
    "            output = model(input)\n",
    "            loss = loss_fn(output, target)\n",
    "\n",
    "        # Backward ops run in the same dtype autocast chose for corresponding forward ops.\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        # scaler.step() first unscales the gradients of the optimizer's assigned params.\n",
    "   \n",
    "        scaler.step(optimizer)\n",
    "\n",
    "        # Updates the scale for next iteration.\n",
    "        scaler.update()\n",
    "        # Unscales the gradients of optimizer's assigned params in-place\n",
    "        scaler.unscale_(optimizer)\n",
    "        # Since the gradients of optimizer's assigned params are unscaled, clips as usual: \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm)    "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Working with scaled gradients\n",
    "\n",
    "### Gradient accumulation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "scaler = GradScaler()\n",
    "\n",
    "for epoch in epochs:\n",
    "    for i, (input, target) in enumerate(data):\n",
    "        with autocast():\n",
    "            output = model(input)\n",
    "            loss = loss_fn(output, target)\n",
    "            # normalize the loss \n",
    "            loss = loss / iters_to_accumulate\n",
    "\n",
    "        # Accumulates scaled gradients.\n",
    "        scaler.scale(loss).backward()\n",
    "          # weights update\n",
    "        if (i + 1) % iters_to_accumulate == 0:\n",
    "            # may unscale_ here if desired \n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Gradient penalty"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "for epoch in epochs:\n",
    "    for input, target in data:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(input)\n",
    "        loss = loss_fn(output, target)\n",
    "\n",
    "        # Creates gradients\n",
    "        grad_prams = torch.autograd.grad(outputs=loss,\n",
    "                                          inputs=model.parameters(),\n",
    "                                          create_graph=True)\n",
    "\n",
    "        # Computes the penalty term and adds it to the loss\n",
    "        grad_norm = 0\n",
    "        for grad in grad_prams:\n",
    "            grad_norm += grad.pow(2).sum()\n",
    "        grad_norm = grad_norm.sqrt()\n",
    "        loss = loss + grad_norm\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        # You can clip gradients here\n",
    "\n",
    "        optimizer.step()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "scaler = GradScaler()\n",
    "\n",
    "for epoch in epochs:\n",
    "    for input, target in data:\n",
    "        optimizer.zero_grad()\n",
    "        with autocast():\n",
    "            output = model(input)\n",
    "            loss = loss_fn(output, target)\n",
    "\n",
    "        # Perform loss scaling for autograd.grad's backward pass, resulting #scaled_grad_prams\n",
    "        scaled_grad_prams = torch.autograd.grad(outputs=scaler.scale(loss),\n",
    "                                                 inputs=model.parameters(),\n",
    "                                                 create_graph=True)\n",
    "\n",
    "        # Creates grad_prams before computing the penalty(grad_prams must be #unscaled). \n",
    "        # Because no optimizer owns scaled_grad_prams, conventional division #is used instead of scaler.unscale_:\n",
    "        inv_scaled = 1./scaler.get_scale()\n",
    "        grad_prams = [p * inv_scaled for p in scaled_grad_prams]\n",
    "\n",
    "        # The penalty term is computed and added to the loss. \n",
    "        with autocast():\n",
    "            grad_norm = 0\n",
    "            for grad in grad_prams:\n",
    "                grad_norm += grad.pow(2).sum()\n",
    "            grad_norm = grad_norm.sqrt()\n",
    "            loss = loss + grad_norm\n",
    "\n",
    "        # Applies scaling to the backward call.\n",
    "        # Accumulates properly scaled leaf gradients.\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        # You can unscale_ here \n",
    "\n",
    "        # step() and update() proceed as usual.\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Working With Multiple Models, Losses, and Optimizers"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "for epoch in epochs:\n",
    "    for input, target in data:\n",
    "        optimizer1.zero_grad()\n",
    "        optimizer2.zero_grad()\n",
    "        with autocast():\n",
    "            output1 = model1(input)\n",
    "            output2 = model2(input)\n",
    "            loss1 = loss_fn(2 * output1 + 3 * output2, target)\n",
    "            loss2 = loss_fn(3 * output1 - 5 * output2, target)\n",
    "\n",
    "       #Although retain graph is unrelated to amp, it is present in this  #example since both backward() calls share certain regions of graph. \n",
    "        scaler.scale(loss1).backward(retain_graph=True)\n",
    "        scaler.scale(loss2).backward()\n",
    "\n",
    "        # If you wish to view or adjust the gradients of the params they #possess, you may specify which optimizers get explicit unscaling. .\n",
    "        scaler.unscale_(optimizer1)\n",
    "\n",
    "        scaler.step(optimizer1)\n",
    "        scaler.step(optimizer2)\n",
    "\n",
    "        scaler.update()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Working with multiple GPUs"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model = Model_m()\n",
    "p_model = nn.DataParallel(model)\n",
    "\n",
    "# Sets autocast in the main thread\n",
    "with autocast():\n",
    "    # There will be autocasting in p_model. \n",
    "    output = p_model(input)\n",
    "    # loss_fn also autocast\n",
    "    loss = loss_fn(output)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}